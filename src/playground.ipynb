{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abadouh/.local/lib/python3.6/site-packages/ipykernel_launcher.py:20: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    " \n",
    "from nltk.cluster import KMeansClusterer\n",
    "import nltk\n",
    "from sklearn import cluster\n",
    "from sklearn import metrics\n",
    " \n",
    "# training data\n",
    " \n",
    "sentences = [['this', 'is', 'the', 'good', 'machine', 'learning', 'book'],\n",
    "            ['this', 'is',  'another', 'book'],\n",
    "            ['one', 'more', 'book'],\n",
    "            ['this', 'is', 'the', 'new', 'post'],\n",
    "          ['this', 'is', 'about', 'machine', 'learning', 'post'],  \n",
    "            ['and', 'this', 'is', 'the', 'last', 'post']]\n",
    " \n",
    "model = Word2Vec(sentences, min_count=1)\n",
    " \n",
    "# get vector data\n",
    "X = model[model.wv.vocab]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__class__', '__contains__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_adapt_by_suffix', '_check_training_sanity', '_clear_post_train', '_do_train_job', '_get_job_params', '_get_thread_working_mem', '_job_producer', '_load_specials', '_log_epoch_end', '_log_epoch_progress', '_log_progress', '_log_train_end', '_minimize_model', '_raw_word_count', '_save_specials', '_set_train_params', '_smart_save', '_train_epoch', '_update_job_params', '_worker_loop', 'accuracy', 'build_vocab', 'build_vocab_from_freq', 'clear_sims', 'cum_table', 'delete_temporary_training_data', 'doesnt_match', 'estimate_memory', 'evaluate_word_pairs', 'get_latest_training_loss', 'hashfxn', 'init_sims', 'intersect_word2vec_format', 'iter', 'layer1_size', 'load', 'load_word2vec_format', 'log_accuracy', 'min_count', 'most_similar', 'most_similar_cosmul', 'n_similarity', 'predict_output_word', 'reset_from', 'sample', 'save', 'save_word2vec_format', 'score', 'similar_by_vector', 'similar_by_word', 'similarity', 'syn0_lockf', 'syn1', 'syn1neg', 'train', 'wmdistance']\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "model = KeyedVectors.load_word2vec_format('path/to/GoogleNews-vectors-negative300.bin', binary=True)\n",
    "model.save_word2vec_format('path/to/GoogleNews-vectors-negative300.txt', binary=False)\n",
    "\n",
    "#print(dir(Word2Vec))\n",
    "\n",
    "#model.wv.most_similar(positive=['woman', 'king'], negative=['man'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "# training model\n",
    "model = Word2Vec(sentences, min_count=1)\n",
    " \n",
    "# get vector data\n",
    "X = model[model.wv.vocab]\n",
    "#print (X)\n",
    " \n",
    "#print (model.wv.similarity('this', 'is'))\n",
    " \n",
    "#print (model.wv.similarity('post', 'book'))\n",
    " \n",
    "#print (model.wv.most_similar(positive=['machine'], negative=[], topn=2))\n",
    " \n",
    "print (len(model['post']))\n",
    " \n",
    "#print (list(model.wv.vocab))\n",
    " \n",
    "print (len(list(model.wv.vocab)))\n",
    " \n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading w2v from pickle\n"
     ]
    }
   ],
   "source": [
    "from W2VFeatures import W2VFeatures\n",
    "\n",
    "W2V = W2VFeatures()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.12270258  0.2709387   0.23694333 -0.25195676 -0.03603756 -0.08606096\n",
      " -0.22414538 -0.03326676  0.2378718   0.18210538  0.0035262   0.12287644\n",
      " -0.2056825  -0.45713335 -0.17697626 -0.0398748   0.14098725  0.41433764\n",
      "  0.16581596 -0.35464868  0.13505137  0.12404513 -0.05210396 -0.01352637\n",
      " -0.40541548 -0.27143618  0.40010083 -0.04837311 -0.2179451  -0.07396073\n",
      " -0.16750178 -0.08748166 -0.19322972 -0.03054483  0.2261186  -0.3920341\n",
      "  0.22259714  0.2789107   0.18378086 -0.04549083 -0.28934804  0.12113429\n",
      " -0.29655105 -0.2250523   0.24398126  0.31844535 -0.03652967  0.26595414\n",
      " -0.22895955  0.31203824  0.1406535  -0.20334537 -0.20894016  0.05412682\n",
      " -0.19672596 -0.06300674 -0.10818047 -0.0110069   0.08477326  0.13235158\n",
      " -0.1035787   0.0352969   0.12128071 -0.01789323 -0.22067867  0.15299575\n",
      "  0.03053425  0.12432688 -0.13341196  0.12746157 -0.02909872  0.2402292\n",
      " -0.10705548  0.38474047  0.12124959  0.21019249  0.23662463 -0.11459363\n",
      " -0.0077008   0.09891534  0.00118749  0.02341386  0.35101274 -0.18458325\n",
      " -0.2309246  -0.06607894 -0.38464037 -0.39813006  0.03867634 -0.02288679\n",
      " -0.51566184  0.10964067  0.13758436  0.31932846  0.37739038  0.20364542\n",
      "  0.05415025  0.07544051  0.16770686  0.14574277 -0.18882005  0.09126314\n",
      "  0.3293147  -0.02752761  0.03656145 -0.03675203  0.04480223 -0.00531772\n",
      " -0.06793455 -0.42332685 -0.19039011 -0.36147353  0.43819544 -0.16567802\n",
      " -0.07154316  0.16863982 -0.20245822  0.01405248 -0.06402738  0.17544776\n",
      "  0.02562176 -0.12933901  0.12695219 -0.01325958 -0.19813463  0.0779809\n",
      "  0.10937625 -0.03831379 -0.05537123  0.13770771  0.12734134  0.23331214\n",
      "  0.07247884  0.29642835 -0.34976244 -0.14447905 -0.19432606 -0.11222792\n",
      " -0.22820468 -0.2252701   0.6060796  -0.17690602 -0.09954038  0.0847149\n",
      " -0.07244963 -0.18348372  0.1897468   0.29539546 -0.06787922 -0.00747078\n",
      " -0.03057387 -0.24160758  0.05271862  0.03713055 -0.18755198 -0.00794828\n",
      " -0.3554667   0.12901783  0.17038125 -0.14896376 -0.22444806 -0.00596819\n",
      " -0.04406502  0.2643393   0.02874295 -0.05022879 -0.50044763 -0.37659058\n",
      " -0.07094555 -0.1674448   0.02869622 -0.17654462  0.40385532 -0.1366359\n",
      " -0.01706573  0.3398638   0.06603207 -0.23524787 -0.27748814 -0.22919239\n",
      "  0.21515079  0.07823353 -0.07241229 -0.13853735  0.28890452  0.21485482\n",
      "  0.21634488  0.09457745  0.20081562 -0.3361726  -0.00228715 -0.0829431\n",
      "  0.08029694  0.02323893  0.00367265  0.16910581  0.26183265  0.17997012\n",
      " -0.00814842 -0.3293561 ]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2351706"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(W2V.get('paracetamol'))\n",
    "W2V.get('paracetamol')\n",
    "len(W2V.w2v_dic.keys())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "NUM_CLUSTERS=3\n",
    "kclusterer = KMeansClusterer(NUM_CLUSTERS, distance=nltk.cluster.util.cosine_distance, repeats=25)\n",
    "assigned_clusters = kclusterer.cluster(X, assign_clusters=True)\n",
    "print (assigned_clusters)\n",
    " \n",
    "\n",
    " \n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = list(model.wv.vocab)\n",
    "for i, word in enumerate(words):  \n",
    "    print (word + \":\" + str(assigned_clusters[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "kmeans = cluster.KMeans(n_clusters=NUM_CLUSTERS)\n",
    "kmeans.fit(X)\n",
    " \n",
    "labels = kmeans.labels_\n",
    "centroids = kmeans.cluster_centers_\n",
    " \n",
    "print (\"Cluster id labels for inputted data\")\n",
    "print (labels)\n",
    "print (\"Centroids data\")\n",
    "print (centroids)\n",
    " \n",
    "print (\"Score (Opposite of the value of X on the K-means objective which is Sum of distances of samples to their closest cluster center):\")\n",
    "print (kmeans.score(X))\n",
    " \n",
    "silhouette_score = metrics.silhouette_score(X, labels, metric='euclidean')\n",
    " \n",
    "print (\"Silhouette_score: \")\n",
    "print (silhouette_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import cluster\n",
    "from sklearn import metrics\n",
    "import wordembedding as we \n",
    "\n",
    "#model = gensim.models.Word2Vec(X, size=100)\n",
    "w2v = dict(zip(model.wv.index2word, model.wv.vectors))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " sentences = [['sa', 'is', 'das', 'good', 'machine', 'learning', 'book'],\n",
    "            ['this', 'is',  'dsa', 'book'],\n",
    "            ['one', 'more', 'rasf'],\n",
    "            ['this', 'is', 'the', 'new', 'post'],\n",
    "          ['this', 'is', 'dadf', 'machine', 'learning', 'post'],  \n",
    "            ['asa', 'bad', 'is', 'yeled']]\n",
    "\n",
    "etree_w2v_tfidf = Pipeline([\n",
    "    (\"word2vec vectorizer\", we.TfidfEmbeddingVectorizer(w2v)),\n",
    "    (\"Kmean\", cluster.KMeans(n_clusters=3))])\n",
    "\n",
    "etree_w2v_tfidf.fit(w2v)\n",
    "print(dir(etree_w2v_tfidf))\n",
    "\n",
    "#etree_w2v_tfidf.predict(w2v)\n",
    "#kmeans = cluster.KMeans(n_clusters=NUM_CLUSTERS)\n",
    "#kmeans.fit(X)\n",
    " \n",
    "#labels = kmeans.labels_\n",
    "#centroids = kmeans.cluster_centers_\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "etree_w2v_tfidf.fit(w2v)\n",
    "print(etree_w2v_tfidf.predict(w2v))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import defaultdict\n",
    "\n",
    "tfidf = TfidfVectorizer(analyzer = 'word')#(analyzer=lambda x: x)\n",
    "tfidf.fit(w2v)\n",
    "max_idf = max(tfidf.idf_)\n",
    "print(max_idf)\n",
    "word2weight = defaultdict(lambda: max_idf, [(w, tfidf.idf_[i]) for w, i in tfidf.vocabulary_.items()])\n",
    "print(word2weight)\n",
    "print(w2v)\n",
    "print(tfidf.vocabulary_.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences1 = [['sa'], ['is']]\n",
    "\n",
    "asaf = we.TfidfEmbeddingVectorizer(w2v)\n",
    "print(asaf.dim)\n",
    "\n",
    "asaf.fit(w2v,\"\")\n",
    "print(asaf.word2weight)\n",
    "asaf.transform(sentences1)\n",
    "\n",
    "#kmc = cluster.KMeans(n_clusters=3)\n",
    "#kmeans = cluster.KMeans(n_clusters=NUM_CLUSTERS)\n",
    "#kmeans.fit(X)\n",
    "\n",
    "#etree_w2v_tfidf.predict(\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(asaf.word2vec[\"is\"]*3.0794415416798357)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.vectors\n",
    "model.wv.index2word\n",
    "w2v = dict(zip(model.wv.index2word, model.wv.vectors))\n",
    "w2v"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
