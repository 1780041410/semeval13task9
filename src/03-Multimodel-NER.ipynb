{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pau/anaconda3/envs/ahltmai/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xx0x0X_x0\n",
      "Xx0x0Xx0\n",
      "Xx0x_0X_x0\n"
     ]
    }
   ],
   "source": [
    "from CustomFeatures import MyFeatures\n",
    "from NERmodel import NERmodel\n",
    "import subprocess\n",
    "import traceback, sys\n",
    "import pickle\n",
    "\n",
    "mf = MyFeatures()\n",
    "print(mf.wordStructure((\"Hell75o3,HA-niacidine0\",)))\n",
    "print(mf.wordStructure2((\"Hell75o3,HA-niacidine0\",)))\n",
    "print(mf.wordStructure((\"Hell75o_3,HA-niacidine0\",)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pau/anaconda3/envs/ahltmai/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature instances:\n",
      "[<CustomFeatures.MyFeatures object at 0x7fc688614128>, <CustomFeatures.MyFeatures object at 0x7fc641d19e48>]\n",
      "\n",
      "models:\n",
      "[<NERmodel.NERmodel object at 0x7fc641d19fd0>, <NERmodel.NERmodel object at 0x7fc642d40be0>]\n",
      "\n",
      "2\n",
      "[None, None]\n",
      "model pickles:\n",
      "['../data/models/svm-pipeline_20180516155814.pkl', '../data/models/svm-pipeline_20180516180415.pkl']\n",
      "\n",
      "model 0:\n",
      "0.0 \n",
      "model 1:\n",
      "0.0 \n"
     ]
    }
   ],
   "source": [
    "# generate feature subsets\n",
    "from CustomFeatures import MyFeatures\n",
    "from NERmodel import NERmodel\n",
    "import subprocess\n",
    "import traceback, sys\n",
    "import pickle\n",
    "\n",
    "\n",
    "# name the batch training\n",
    "training_set_name = \"singleton20180516\"\n",
    "\n",
    "# preapre different MyFeature instances\n",
    "fs = []\n",
    "# number of model permutations\n",
    "m = 1\n",
    "# original model with all features\n",
    "mf = MyFeatures()\n",
    "fs.append(mf)\n",
    "# for now it is random shuffling, but this is gonna be a manual selection of features\n",
    "for i in range(m):\n",
    "    try:\n",
    "        mf = MyFeatures()\n",
    "        mf.deriveNewFeatureSet(degree=3)\n",
    "        fs.append(mf)\n",
    "        #mf.printActiveFeatureFunctions()\n",
    "        #print()\n",
    "    except:\n",
    "        pass\n",
    "        \n",
    "print(\"feature instances:\")\n",
    "print(fs)\n",
    "print()    \n",
    "\n",
    "# initialize\n",
    "models=[]\n",
    "for fset in fs:\n",
    "    try:\n",
    "        model = NERmodel(featureset=fset)\n",
    "        model.setName(training_set_name)\n",
    "        models.append(model)\n",
    "    except:\n",
    "        pass\n",
    "print(\"models:\")\n",
    "print(models)\n",
    "print()\n",
    "    \n",
    "\n",
    "# train all models\n",
    "models_pickles =[None]*len(models)\n",
    "print(len(models_pickles))\n",
    "print(models_pickles)\n",
    "for i in range(len(models)):\n",
    "    model = models[i]\n",
    "    try:\n",
    "        model.trainFeatureExtraction(\"../data/LaboCase/Train\", limit=None)\n",
    "        model.saveTrainingFeatures(\"../data/models/multitrain-data\"+str(i)+\".json\")\n",
    "        model.newModelPipeline()\n",
    "        models_pickles[i] = model.saveModelPipeline()\n",
    "    except Exception:\n",
    "        print(\"Exception in user code:\",i)\n",
    "        print(\"-\"*60)\n",
    "        traceback.print_exc(file=sys.stdout)\n",
    "        print(\"-\"*60)\n",
    "        print(\"model \"+str(i)+\" has failed... \")\n",
    "        models_pickles[i] = None\n",
    "        \n",
    "print(\"model pickles:\")\n",
    "print(models_pickles)\n",
    "print()\n",
    "\n",
    "# test all models\n",
    "# PROBLEM I need the same feature set! so maybe the \n",
    "# pipeline should be pickled with the featureset\n",
    "# so maybe we should use pickle of the whole NER?\n",
    "results_files = [None]*len(models)\n",
    "evaluation_files = [None]*len(models)\n",
    "for i in range(len(models_pickles)):\n",
    "    modelfile = models_pickles[i]\n",
    "    fset = fs[i]\n",
    "    try:\n",
    "        model = NERmodel(featureset=fs[i])\n",
    "        model.loadModelPipeline(filepath=modelfile)\n",
    "        model.testFeatureExtraction(\n",
    "            '../data/LaboCase/Test/Test for DrugNER task/DrugBank', \n",
    "            limit=None)\n",
    "        model.predict()\n",
    "        model.parsePredictionOutput('../data/models/task9.1_'+training_set_name+'_'+str(i)+'.txt')\n",
    "        results_files[i] = '../data/models/multitrain-'+str(i)+'-output.csv'\n",
    "        print(\"model \"+str(i)+\":\")\n",
    "        evaluation_files[i] = model.autoEvaluation()\n",
    "    except  Exception:\n",
    "        print(\"Exception in user code:\",i)\n",
    "        print(\"-\"*60)\n",
    "        traceback.print_exc(file=sys.stdout)\n",
    "        print(\"-\"*60)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"model predictions:\")\n",
    "print(results_files)\n",
    "print(evaluation_files)\n",
    "print()\n",
    "\n",
    "# print all result sets\n",
    "for i in range(len(results_files)):\n",
    "    file = results_files[i]\n",
    "    file2 = evaluation_files[i]\n",
    "    print(\"model \"+ str(i)+ \"-------------------------------------------------------------------\")\n",
    "    print(open(file,'r').read())\n",
    "    print(open(file2,'r').read())\n",
    "    print(\"-------------------------------------------------------------------\")\n",
    "    print()\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ahltmai",
   "language": "python",
   "name": "ahltmai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
