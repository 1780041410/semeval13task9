{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abadouh/.local/lib/python3.6/site-packages/ipykernel_launcher.py:20: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    " \n",
    "from nltk.cluster import KMeansClusterer\n",
    "import nltk\n",
    "from sklearn import cluster\n",
    "from sklearn import metrics\n",
    " \n",
    "# training data\n",
    " \n",
    "sentences = [['this', 'is', 'the', 'good', 'machine', 'learning', 'book'],\n",
    "            ['this', 'is',  'another', 'book'],\n",
    "            ['one', 'more', 'book'],\n",
    "            ['this', 'is', 'the', 'new', 'post'],\n",
    "          ['this', 'is', 'about', 'machine', 'learning', 'post'],  \n",
    "            ['and', 'this', 'is', 'the', 'last', 'post']]\n",
    " \n",
    "model = Word2Vec(sentences, min_count=1)\n",
    " \n",
    "# get vector data\n",
    "X = model[model.wv.vocab]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__class__', '__contains__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_adapt_by_suffix', '_check_training_sanity', '_clear_post_train', '_do_train_job', '_get_job_params', '_get_thread_working_mem', '_job_producer', '_load_specials', '_log_epoch_end', '_log_epoch_progress', '_log_progress', '_log_train_end', '_minimize_model', '_raw_word_count', '_save_specials', '_set_train_params', '_smart_save', '_train_epoch', '_update_job_params', '_worker_loop', 'accuracy', 'build_vocab', 'build_vocab_from_freq', 'clear_sims', 'cum_table', 'delete_temporary_training_data', 'doesnt_match', 'estimate_memory', 'evaluate_word_pairs', 'get_latest_training_loss', 'hashfxn', 'init_sims', 'intersect_word2vec_format', 'iter', 'layer1_size', 'load', 'load_word2vec_format', 'log_accuracy', 'min_count', 'most_similar', 'most_similar_cosmul', 'n_similarity', 'predict_output_word', 'reset_from', 'sample', 'save', 'save_word2vec_format', 'score', 'similar_by_vector', 'similar_by_word', 'similarity', 'syn0_lockf', 'syn1', 'syn1neg', 'train', 'wmdistance']\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "model = KeyedVectors.load_word2vec_format('path/to/GoogleNews-vectors-negative300.bin', binary=True)\n",
    "model.save_word2vec_format('path/to/GoogleNews-vectors-negative300.txt', binary=False)\n",
    "\n",
    "#print(dir(Word2Vec))\n",
    "\n",
    "#model.wv.most_similar(positive=['woman', 'king'], negative=['man'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "# training model\n",
    "model = Word2Vec(sentences, min_count=1)\n",
    " \n",
    "# get vector data\n",
    "X = model[model.wv.vocab]\n",
    "#print (X)\n",
    " \n",
    "#print (model.wv.similarity('this', 'is'))\n",
    " \n",
    "#print (model.wv.similarity('post', 'book'))\n",
    " \n",
    "#print (model.wv.most_similar(positive=['machine'], negative=[], topn=2))\n",
    " \n",
    "print (len(model['post']))\n",
    " \n",
    "#print (list(model.wv.vocab))\n",
    " \n",
    "print (len(list(model.wv.vocab)))\n",
    " \n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from W2VFeatures import W2VFeatures\n",
    "\n",
    "W2V = W2VFeatures()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "NUM_CLUSTERS=3\n",
    "kclusterer = KMeansClusterer(NUM_CLUSTERS, distance=nltk.cluster.util.cosine_distance, repeats=25)\n",
    "assigned_clusters = kclusterer.cluster(X, assign_clusters=True)\n",
    "print (assigned_clusters)\n",
    " \n",
    "\n",
    " \n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = list(model.wv.vocab)\n",
    "for i, word in enumerate(words):  \n",
    "    print (word + \":\" + str(assigned_clusters[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "kmeans = cluster.KMeans(n_clusters=NUM_CLUSTERS)\n",
    "kmeans.fit(X)\n",
    " \n",
    "labels = kmeans.labels_\n",
    "centroids = kmeans.cluster_centers_\n",
    " \n",
    "print (\"Cluster id labels for inputted data\")\n",
    "print (labels)\n",
    "print (\"Centroids data\")\n",
    "print (centroids)\n",
    " \n",
    "print (\"Score (Opposite of the value of X on the K-means objective which is Sum of distances of samples to their closest cluster center):\")\n",
    "print (kmeans.score(X))\n",
    " \n",
    "silhouette_score = metrics.silhouette_score(X, labels, metric='euclidean')\n",
    " \n",
    "print (\"Silhouette_score: \")\n",
    "print (silhouette_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import cluster\n",
    "from sklearn import metrics\n",
    "import wordembedding as we \n",
    "\n",
    "#model = gensim.models.Word2Vec(X, size=100)\n",
    "w2v = dict(zip(model.wv.index2word, model.wv.vectors))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " sentences = [['sa', 'is', 'das', 'good', 'machine', 'learning', 'book'],\n",
    "            ['this', 'is',  'dsa', 'book'],\n",
    "            ['one', 'more', 'rasf'],\n",
    "            ['this', 'is', 'the', 'new', 'post'],\n",
    "          ['this', 'is', 'dadf', 'machine', 'learning', 'post'],  \n",
    "            ['asa', 'bad', 'is', 'yeled']]\n",
    "\n",
    "etree_w2v_tfidf = Pipeline([\n",
    "    (\"word2vec vectorizer\", we.TfidfEmbeddingVectorizer(w2v)),\n",
    "    (\"Kmean\", cluster.KMeans(n_clusters=3))])\n",
    "\n",
    "etree_w2v_tfidf.fit(w2v)\n",
    "print(dir(etree_w2v_tfidf))\n",
    "\n",
    "#etree_w2v_tfidf.predict(w2v)\n",
    "#kmeans = cluster.KMeans(n_clusters=NUM_CLUSTERS)\n",
    "#kmeans.fit(X)\n",
    " \n",
    "#labels = kmeans.labels_\n",
    "#centroids = kmeans.cluster_centers_\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "etree_w2v_tfidf.fit(w2v)\n",
    "print(etree_w2v_tfidf.predict(w2v))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import defaultdict\n",
    "\n",
    "tfidf = TfidfVectorizer(analyzer = 'word')#(analyzer=lambda x: x)\n",
    "tfidf.fit(w2v)\n",
    "max_idf = max(tfidf.idf_)\n",
    "print(max_idf)\n",
    "word2weight = defaultdict(lambda: max_idf, [(w, tfidf.idf_[i]) for w, i in tfidf.vocabulary_.items()])\n",
    "print(word2weight)\n",
    "print(w2v)\n",
    "print(tfidf.vocabulary_.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences1 = [['sa'], ['is']]\n",
    "\n",
    "asaf = we.TfidfEmbeddingVectorizer(w2v)\n",
    "print(asaf.dim)\n",
    "\n",
    "asaf.fit(w2v,\"\")\n",
    "print(asaf.word2weight)\n",
    "asaf.transform(sentences1)\n",
    "\n",
    "#kmc = cluster.KMeans(n_clusters=3)\n",
    "#kmeans = cluster.KMeans(n_clusters=NUM_CLUSTERS)\n",
    "#kmeans.fit(X)\n",
    "\n",
    "#etree_w2v_tfidf.predict(\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(asaf.word2vec[\"is\"]*3.0794415416798357)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.vectors\n",
    "model.wv.index2word\n",
    "w2v = dict(zip(model.wv.index2word, model.wv.vectors))\n",
    "w2v"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
